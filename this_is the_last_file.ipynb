{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhishek-paghdal/Content_Generation/blob/main/this_is%20the_last_file.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQT-GQdnUFa6",
        "outputId": "bc17826c-b34c-46e8-ae78-b53f9b5de11a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xY7ihc3jvKc-",
        "outputId": "f9745dc4-edab-4dba-c582-c5066363d483"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Required Importing**"
      ],
      "metadata": {
        "id": "lL1IceI8tj5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "import torch\n",
        "import os"
      ],
      "metadata": {
        "id": "D21mvRsi7ifK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Uploading Dataset**"
      ],
      "metadata": {
        "id": "heGzA-YlubAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/Content_Project/data.csv')"
      ],
      "metadata": {
        "id": "kzlOWrmVr0ty"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ziMzL_X7r7HY",
        "outputId": "3d3aef5a-098c-441b-c177-a67848a9b203"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                    _id  \\\n",
              "0  {'$oid': '600837a2b5425d67582e0d8a'}   \n",
              "1  {'$oid': '600837a3b5425d67582e0d8e'}   \n",
              "2  {'$oid': '600837a4b5425d67582e0d91'}   \n",
              "3  {'$oid': '600837a4b5425d67582e0d92'}   \n",
              "4  {'$oid': '600837a5b5425d67582e0d97'}   \n",
              "\n",
              "                                               title        datePublished  \\\n",
              "0  EU lawmakers to push audio-visual sector on ge...  2020-11-30 05:46:35   \n",
              "1  Construction startup Scaled Robotics raises a ...  2020-02-03 07:13:18   \n",
              "2  Roblox buys digital avatar startup Loom.ai – T...  2020-12-14 11:10:15   \n",
              "3  ClickUp hits $1 billion valuation in $100M Ser...  2020-12-15 16:30:14   \n",
              "4  Spotify launches video podcasts worldwide, sta...  2020-07-21 07:13:39   \n",
              "\n",
              "                                         description  \\\n",
              "0  European Union lawmakers are considering wheth...   \n",
              "1  Industrial robots are expensive. But, then, so...   \n",
              "2  Roblox announced today that it’s buying a digi...   \n",
              "3  Just six month after raising its first bit of ...   \n",
              "4  Spotify today announced the global launch of v...   \n",
              "\n",
              "                                             article  \\\n",
              "0  European Union lawmakers are considering wheth...   \n",
              "1  Industrial robots are expensive. But, then, so...   \n",
              "2  Roblox announced today that it’s buying a digi...   \n",
              "3  Just six month after raising its first bit of ...   \n",
              "4  Spotify today announced the global launch of v...   \n",
              "\n",
              "                                            keywords  \n",
              "0  audio-visual, eu, geoblocking, Netflix, stream...  \n",
              "1  Battlefield, construction, Norwegian Construct...  \n",
              "2  Avatar, Bitmoji, bitstrips, computing, films, ...  \n",
              "3  Android, ClickUp, Craft Ventures, Georgian Par...  \n",
              "4           Spotify, streaming music, Video, YouTube  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-171cf3e6-d8a4-4a95-8ff8-cbfa5b9cfb78\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>_id</th>\n",
              "      <th>title</th>\n",
              "      <th>datePublished</th>\n",
              "      <th>description</th>\n",
              "      <th>article</th>\n",
              "      <th>keywords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>{'$oid': '600837a2b5425d67582e0d8a'}</td>\n",
              "      <td>EU lawmakers to push audio-visual sector on ge...</td>\n",
              "      <td>2020-11-30 05:46:35</td>\n",
              "      <td>European Union lawmakers are considering wheth...</td>\n",
              "      <td>European Union lawmakers are considering wheth...</td>\n",
              "      <td>audio-visual, eu, geoblocking, Netflix, stream...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>{'$oid': '600837a3b5425d67582e0d8e'}</td>\n",
              "      <td>Construction startup Scaled Robotics raises a ...</td>\n",
              "      <td>2020-02-03 07:13:18</td>\n",
              "      <td>Industrial robots are expensive. But, then, so...</td>\n",
              "      <td>Industrial robots are expensive. But, then, so...</td>\n",
              "      <td>Battlefield, construction, Norwegian Construct...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>{'$oid': '600837a4b5425d67582e0d91'}</td>\n",
              "      <td>Roblox buys digital avatar startup Loom.ai – T...</td>\n",
              "      <td>2020-12-14 11:10:15</td>\n",
              "      <td>Roblox announced today that it’s buying a digi...</td>\n",
              "      <td>Roblox announced today that it’s buying a digi...</td>\n",
              "      <td>Avatar, Bitmoji, bitstrips, computing, films, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>{'$oid': '600837a4b5425d67582e0d92'}</td>\n",
              "      <td>ClickUp hits $1 billion valuation in $100M Ser...</td>\n",
              "      <td>2020-12-15 16:30:14</td>\n",
              "      <td>Just six month after raising its first bit of ...</td>\n",
              "      <td>Just six month after raising its first bit of ...</td>\n",
              "      <td>Android, ClickUp, Craft Ventures, Georgian Par...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>{'$oid': '600837a5b5425d67582e0d97'}</td>\n",
              "      <td>Spotify launches video podcasts worldwide, sta...</td>\n",
              "      <td>2020-07-21 07:13:39</td>\n",
              "      <td>Spotify today announced the global launch of v...</td>\n",
              "      <td>Spotify today announced the global launch of v...</td>\n",
              "      <td>Spotify, streaming music, Video, YouTube</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-171cf3e6-d8a4-4a95-8ff8-cbfa5b9cfb78')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-171cf3e6-d8a4-4a95-8ff8-cbfa5b9cfb78 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-171cf3e6-d8a4-4a95-8ff8-cbfa5b9cfb78');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Preprocessing Dataset**"
      ],
      "metadata": {
        "id": "zYY2o8sXuqxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking if NULL rows are present\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLneREzBuzYK",
        "outputId": "9c88f51d-32e3-4fc9-9d0e-2146a1ef73c5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_id                0\n",
              "title              0\n",
              "datePublished      0\n",
              "description        0\n",
              "article           21\n",
              "keywords         919\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing the row with null values\n",
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "1JQvsT44r_L9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Making new dataset \n",
        "df1=df"
      ],
      "metadata": {
        "id": "-BGCUh5qsGUE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.to_csv('data1.csv',index=False)"
      ],
      "metadata": {
        "id": "FUq2eKfXsMV7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv('data1.csv')"
      ],
      "metadata": {
        "id": "1wXKrlHUv8YD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOBjvWgvwMHf",
        "outputId": "cca43aa8-8a1f-4f1b-f57f-e9b998c77d02"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24380, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing duplicates rows\n",
        "data.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "gPz3wxXfwkVS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Now check the language of texts**"
      ],
      "metadata": {
        "id": "CzICdAYnxIbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7soyMUNxBxX",
        "outputId": "31907447-b22f-4416-fc23-558ad49511dc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.4/981.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993243 sha256=c41f06376a73a2cb968724f13877b2df8550ef16cb590bb8a59623d2e2f79841\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/c1/d9/7e068de779d863bc8f8fc9467d85e25cfe47fa5051fff1a1bb\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langdetect"
      ],
      "metadata": {
        "id": "luJsrQDrxU-b"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_english(text):\n",
        "    try:\n",
        "        lang = langdetect.detect(text)\n",
        "        if lang == 'en':\n",
        "            return True\n",
        "        else:\n",
        "            return False\n",
        "    except:\n",
        "        return False"
      ],
      "metadata": {
        "id": "z3R2pWp2xZE3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['is_english'] = data['article'].apply(lambda x: is_english(x))"
      ],
      "metadata": {
        "id": "u4cvwaQqxoNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "_-MivJw3yKFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['is_english'].value_counts()"
      ],
      "metadata": {
        "id": "jUpnONWjyTVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data= data[data['is_english']==True]"
      ],
      "metadata": {
        "id": "4lYATsCXy2Ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Text Length Count**"
      ],
      "metadata": {
        "id": "DYobCoZZzB6A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter"
      ],
      "metadata": {
        "id": "bqDbiuWzzFnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['text_length'] = data['article'].apply(len)"
      ],
      "metadata": {
        "id": "5Rs8DIoTzRTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.sort_values(by='text_length')"
      ],
      "metadata": {
        "id": "oYvKzcLbzmnu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Now consider articles with length between (300 and 30000)**"
      ],
      "metadata": {
        "id": "sBtxDrRt1QF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data= data[(data['text_length'] >= 300) & (data['text_length'] <= 20000)]"
      ],
      "metadata": {
        "id": "_170Cm6K1Ngs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "sKyVV3T42LJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Split the dataset into train and evaluate**"
      ],
      "metadata": {
        "id": "9QyfG0fuvk2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, eval_data = train_test_split(data, test_size=0.2)"
      ],
      "metadata": {
        "id": "9eLn5dNLvrID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "metadata": {
        "id": "nM6EwJs3wrL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.to_csv('train_data.csv')\n",
        "eval_data.to_csv('eval_data.csv')"
      ],
      "metadata": {
        "id": "zPvJgszewvfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this is for the train data\n",
        "\n",
        "# Open the CSV file and read the contents\n",
        "with open('/content/train_data.csv', 'r', encoding='utf-8') as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    data = [row['article'] for row in reader]\n",
        "\n",
        "# Write the contents to a text file\n",
        "with open('/content/train_dataset.txt', 'w', encoding='utf-8') as txtfile:\n",
        "    txtfile.write('\\n'.join(data))\n"
      ],
      "metadata": {
        "id": "zWhtJPb6rQeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this is for the evaluate data\n",
        "\n",
        "# Open the CSV file and read the contents\n",
        "with open('/content/eval_data.csv', 'r', encoding='utf-8') as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    data = [row['article'] for row in reader]\n",
        "\n",
        "# Write the contents to a text file\n",
        "with open('/content/eval_dataset.txt', 'w', encoding='utf-8') as txtfile:\n",
        "    txtfile.write('\\n'.join(data))\n"
      ],
      "metadata": {
        "id": "fTOqWtRMxB6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Load the model and tokenizer**"
      ],
      "metadata": {
        "id": "6nYazgvR25dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def load_model(model_dir):\n",
        "#     tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "#     model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "#     # Check if a model exists in the given directory\n",
        "#     model_path = model_dir + \"/model.pth\"\n",
        "#     if os.path.exists(model_path):\n",
        "#         # Load the model\n",
        "#         model.load_state_dict(torch.load(model_path))\n",
        "#         print(\"Loaded model from:\", model_path)\n",
        "#     else:\n",
        "#         print(\"No model found at:\", model_path)\n",
        "\n",
        "#     return model, tokenizer"
      ],
      "metadata": {
        "id": "YcuGu5_BRvEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model,tokenizer=load_model('/content/drive/MyDrive/GPT2_saved_model')"
      ],
      "metadata": {
        "id": "Xu8x_i94R36c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained GPT-2 tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')"
      ],
      "metadata": {
        "id": "72GRRK53thcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "for name, param in model.named_parameters():\n",
        "    if 'transformer.h.11' in name or 'transformer.h.10' in name:\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "        param.requires_grad = False\n"
      ],
      "metadata": {
        "id": "nKwrzFPGqQYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if not param.requires_grad:\n",
        "        print(name)\n"
      ],
      "metadata": {
        "id": "BkSv1JIWo3yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, child in model.named_children():\n",
        "    print(name, 'requires_grad:', any(param.requires_grad for param in child.parameters()))\n",
        "\n"
      ],
      "metadata": {
        "id": "tZwP6Tc0qDhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Prepare your dataset by creating a TextDataset object**"
      ],
      "metadata": {
        "id": "sPhgTgKI3FGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare your dataset by creating a TextDataset object\n",
        "train_dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path='/content/train_dataset.txt',\n",
        "    block_size=128\n",
        ")"
      ],
      "metadata": {
        "id": "teLEUf84tmnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare your dataset by creating a TextDataset object\n",
        "eval_dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path='/content/eval_dataset.txt',\n",
        "    block_size=128\n",
        ")\n"
      ],
      "metadata": {
        "id": "6XwMGM9axeRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**checkpoint using WANDB**"
      ],
      "metadata": {
        "id": "NC3_UjxCHTRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wandb"
      ],
      "metadata": {
        "id": "xI2nWMsDJVXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers.integrations import WandbCallback"
      ],
      "metadata": {
        "id": "EAR3-x-zJ4jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up WANB\n",
        "wandb.login()\n",
        "wandb.init(project=\"my_project_tech_articles\")\n"
      ],
      "metadata": {
        "id": "-GRF3zMZHfOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Set up the trainer with the appropriate training arguments**"
      ],
      "metadata": {
        "id": "BD-j314IH9pL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='.\\results',\n",
        "    evaluation_strategy='epoch',\n",
        "    learning_rate=0.0001,\n",
        "    per_device_train_batch_size=4,\n",
        "    num_train_epochs=1,\n",
        "    save_total_limit=2,\n",
        "    save_steps=62,\n",
        "    resume_from_checkpoint=True,\n",
        "    weight_decay=0.001,\n",
        "    report_to='wandb',  # Enable WandbCallback\n",
        "   # load_best_model_at_end=True  # load best model checkpoint at the end\n",
        ")\n",
        "\n",
        "# Define the WandbCallback\n",
        "wandb_callback = WandbCallback()\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "# Define the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=data_collator,\n",
        "    callbacks=[WandbCallback()],\n",
        ")"
      ],
      "metadata": {
        "id": "NbUFrSDmHwXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Train the model**"
      ],
      "metadata": {
        "id": "OyTXUzU34V5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "ddV9ZgkvaiVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_grad_enabled(True)"
      ],
      "metadata": {
        "id": "NrvhInYalQtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "p4oA22F84sCf",
        "outputId": "c13e7428-d3cd-497a-c7ac-2c629d674116"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3435b262f1ae>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_0wRMyM_SDoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "-vBn8rpy5usv",
        "outputId": "7d052232-d33a-4f19-d606-688b3440dfec"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='82' max='82' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [82/82 00:06]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 3.5319020748138428,\n",
              " 'eval_runtime': 6.9039,\n",
              " 'eval_samples_per_second': 95.018,\n",
              " 'eval_steps_per_second': 11.877,\n",
              " 'epoch': 5.0}"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Save the best model checkpoint\n",
        "# trainer.save_model('/path/to/save/directory')\n"
      ],
      "metadata": {
        "id": "l_x-ThgBIJlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Now code to rehit the promt to get required length article**"
      ],
      "metadata": {
        "id": "eJaqp0T24yeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set the maximum length of each segment\n",
        "max_segment_length = 1024\n",
        "\n",
        "# Set the initial prompt\n",
        "prompt = \"Deep learning \"\n",
        "\n",
        "# Initialize the generated text\n",
        "generated_text = prompt\n",
        "\n",
        "\n",
        "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        " # Generate the next segment of text\n",
        "output = model.generate(input_ids=input_ids.to('cuda'),\n",
        "                             max_length=max_segment_length,\n",
        "                             do_sample=True,\n",
        "                             top_p=0.95,\n",
        "                             temperature=0.7,\n",
        "                             top_k=50,\n",
        "                             num_beams=5\n",
        "                        \n",
        "                             \n",
        "    \n",
        "    # Decode the output to text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "# Print the generated text\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xti9pCt_aYZ",
        "outputId": "6d9d08af-b440-4bf5-f195-df008c132183"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep learning  from Google’s new algorithm  of machine learning  about the human mind. This new algorithm will lead to a smarter AI,” that can better understand and understand human emotions,” and give us an accurate and accurate picture of what’s going on in our heads. The machine learning algorithms will be able to understand human emotions,” and predict their behavior,” even if they’re not in the right mood to do so. The algorithm will help us understand the emotions of our people,” and they will be able to use the algorithms to better inform us about their mental states. The AI will also learn how to use its information to help us in our daily lives,” and it will learn to distinguish between emotion and information in a more intuitive way. For instance, the AI will be able to learn to distinguish between a human’s words,” and a computer’s action,” and also to distinguish between different kinds of emotions.” These algorithms will be able to predict which emotions are most important, and which are least important, to our happiness and happiness-related problems. The AI will also be able to use its information to help us improve our health and mental health,” and to help us to protect our personal health,” and to protect the safety of our homes and workplaces. The AI will be able to improve our health and mental health,” and it will help us improve our relationships,” and better our relationships with other people,” and better our relationships with other people. The AI will also be able to change the way people are treated in society,” and better their health,” and their health-related problems. The AI will also be able to improve their relationships with other people, and better their relationships with other people. The AI will also be able to help us improve the quality of our work,” and improve our relationships with other people, and better our relationships with other people. And the AI will help us to improve our relationships with other people, and better our relationships with other people. This technology will allow the AI to work on a number of fronts:” it will help it learn from human mistakes,” it will help it learn from people’s mistakes, and it will help it learn from people’s own mistakes. The AI will also help it work on a number of fronts: it will help it learn from human mistakes,” it will help it learn from people’s mistakes, and it will help it learn from people’s own mistakes. In addition, it will help it work on a number of fronts: it will help it learn from the people who are more likely to use it, and it will help it learn from the people who are more likely to use it. We’ve already seen that artificial intelligence, which is often used in medicine, is helping doctors diagnose and treat many conditions. Now, we know that the human brain is developing a set of new abilities, so that it can help doctors diagnose and treat a wide range of conditions, including: cancer, diabetes, heart disease, diabetes mellitus, HIV/AIDS, cancer, arthritis, Alzheimer’s disease, neurodegenerative diseases, and many more. The AI will also help us understand the emotional and cognitive aspects of our lives. It will help us understand the emotions of our people, and it will help us to understand the emotions of our homes and workplaces. It will help us understand the emotional and cognitive aspects of our lives. In addition, it will help us understand the emotions of our people, and it will help us to understand the emotions of our homes and workplaces. The AI will also help us understand the emotions of our homes and workplaces. It will help us understand the emotions of our homes and workplaces. The AI will help us understand the emotions of our homes and workplaces. The AI will help us understand the emotions of our homes and workplaces. The AI will help us understand the emotions of our homes and workplaces. The AI will help us understand the emotions of our homes and workplaces. The AI will help us understand the emotions of our homes and workplaces. And the AI will help us understand the emotions of our homes and workplaces. The AI will help us understand the emotions of our homes and workplaces. And the AI will help us understand the emotions of our homes and workplaces. And the AI will help us understand the emotions of our homes and workplaces. “The AI is also going to help us understand and evaluate our own emotions and emotions. It is also going to help us understand and evaluate our own emotions and emotions. “It will also help us understand and evaluate our own emotions and emotions. “It will also help us understand and evaluate our own emotions and emotions.” And we’ve already seen that artificial intelligence, which is often used in medicine, is helping doctors diagnose and treat many conditions. Now, we know that\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set the maximum length of each segment\n",
        "max_segment_length = 512\n",
        "\n",
        "# Set the initial prompt\n",
        "prompt = \"Tesla automated vehicles\"\n",
        "\n",
        "# Initialize the generated text\n",
        "generated_text = prompt\n",
        "\n",
        "# Generate segments of text and stitch them together\n",
        "while len(generated_text) < 500:\n",
        "    # Encode the prompt as input IDs\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "\n",
        "    # Generate the next segment of text\n",
        "    output = model.generate(input_ids=input_ids.to('cuda'),\n",
        "                             max_length=max_segment_length,\n",
        "                             do_sample=True,\n",
        "                             top_p=0.95,\n",
        "                             temperature=0.7,\n",
        "                             top_k=50)\n",
        "    \n",
        "    # Decode the output to text\n",
        "    segment = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Add the segment to the generated text\n",
        "    generated_text += segment\n",
        "\n",
        "    # Get the last 1024 tokens of the previous segment as the new prompt\n",
        "    prompt = generated_text[-max_segment_length:]\n",
        "\n",
        "# Print the generated text\n",
        "print(generated_text)\n",
        "final_text=generated_text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcab_ZbE4cyO",
        "outputId": "62cc9efc-f7b6-409c-c1a2-eff396ebf1ba"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla automated vehiclesTesla automated vehicles on public roads, such as public transportation and public transit, and public transport systems, such as taxis and Uber, have been a key driver of the industry, the company said in a statement. The company said it will continue to invest in autonomous vehicles, which it is calling a “high-growth opportunity,” said the announcement. “The company said it expects to raise more capital this year than in the past, noting that it expects to increase its capital expenditures by 10% this year. “The company said it expects to make $4.2 billion in capital expenditures this year, up from $2.7 billion in the same period last year, while it expects to raise $8.9 billion in capital expenditures this year, down from $9.1 billion in the same period in the same period last year. It also expects to invest more than $5 billion in the next three years, up from $4.7 billion in the same period last year.” The new investments in autonomous vehicles, which it said will be “the biggest investment in the next three years, will allow it to invest in the startup ecosystem and accelerate the development of driverless vehicles, the company said. “The company’s capital expenditures in the next three years are up from $1.2 billion in the same period last year, while its operating profit is up from $1.4 billion in the same period last year. “The new investments in autonomous vehicles, which it said will be “the biggest investment in the next three years, will allow it to invest in the startup ecosystem and accelerate the development of driverless vehicles, the company said.  Future investments in autonomous vehicles have the potential to significantly accelerate the adoption of self-driving cars in the U.S., which has seen growth in the number of people using them, and in the number of vehicles equipped with advanced safety systems. The company said it plans to invest $2 billion this year to accelerate the development of driverless vehicles, which it expects to continue to invest in the industry. In April, Tesla said it would  focus on new technology, including new autonomous vehicles, to accelerate the adoption of autonomous vehicles, which it said will bring down the cost of car ownership. It also  plans to invest $1 billion this year to accelerate the development of driverless vehicles, which it expects to continue to invest in the industry. The company also plans to invest $1 billion this year to accelerate the development of driver\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(final_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4fP7gY-Coe0",
        "outputId": "32aedfd2-bd1b-4389-c8e0-d31187024ec7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1592\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(final_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ou4b4KxBhcE",
        "outputId": "8aaa46f4-ab42-4e5b-cf62-ef55a94d3566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Use text summarization to improve article**"
      ],
      "metadata": {
        "id": "7n9X7I2l7YJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "#!pip install transformers\n",
        "\n",
        "# Import required libraries\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load pre-trained summarization model\n",
        "#summarizer = pipeline(\"summarization\")\n",
        "# Load pre-trained summarization model (using BART-Large-CNN)\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "\n",
        "# Define input text to summarize\n",
        "input_text = \"The United States is a country located in North America. It is the third largest country in the world by total area and the third most populous country in the world. The United States has a diverse geography and climate, with mountains, forests, deserts, and coastal areas. Its economy is the largest in the world, driven by industries such as technology, finance, and healthcare.\"\n",
        "\n",
        "# Generate summary for input text\n",
        "summary = summarizer(final_text, max_length=1000, min_length=30, do_sample=False)\n",
        "\n",
        "# Print generated summary\n",
        "print(summary[0]['summary_text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFrW7OKK6NWf",
        "outputId": "0d527d01-85fc-42ca-964b-325c91df7cb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Your max_length is set to 1000, but you input_length is only 304. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=152)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New tools are being made available to developers through the “Digital Machines’ platform, which is open source and accessible to the public. Developers can “use the Digital Machines” platform to connect with a number of existing and emerging AI capabilities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lntcava433_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WNuQ0-6m34R0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uu1hCaVK34cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kqlg5pKBC0hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q3MKq5AoC0kK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qNSbrQWVC0m_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Below code is for save the model**"
      ],
      "metadata": {
        "id": "LDQJe0fq6OcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model.to('cuda')a# Convert the model to CPU\n",
        "\n",
        "model = model.to(torch.device('cpu'))\n"
      ],
      "metadata": {
        "id": "b57VIFZN_T4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate text using the fine-tuned model\n",
        "prompt = 'which is the best phone that i can buy ?'\n",
        "generated_text = model.generate(\n",
        "    input_ids=tokenizer.encode(prompt, return_tensors='pt'),\n",
        "    max_length=200,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    temperature=1.0,\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(generated_text[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5dUnEdWt6nP",
        "outputId": "c8b62e66-5a71-4213-a3a6-3e705a3f51e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "which is the best phone that i can buy? ) and other  Android based devices  in a year, it also’s a fairly large portfolio that also features more than 100,000 square feet of available office space. The company offers the new HomePod speaker, Alexa speaker and Alexa-powered streaming and  cloud services  in addition to the usual smart home accessories and accessories. In terms of the product, it’s not really great, though it’s a small handset that  packs in some pretty nice battery life. The only real complaint I have with the phone is its camera, which the company did provide in an in-person demo. There is a built-in Wi-Fi hotspot to minimize the risk that you might connect to a neighboring area but there are no hotspots available on the device. However, with its smaller screen size, the HomePod speaker isn’t far behind the competition, and if you want to bring a full pair of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.save_model('Second_Model_GPT2-Epoch_3_1000')"
      ],
      "metadata": {
        "id": "-GZrLfH4X2G9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import zipfile\n",
        "# zip_ref = zipfile.ZipFile(\"Model_2_GPT-2_Epoch_3_1000.zip\", \"r\")\n",
        "# zip_ref.extractall()\n",
        "# zip_ref.close()"
      ],
      "metadata": {
        "id": "QJUIMeuH_ugz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# joblib.dump(model, 'SecondModel_Epoch_3_1000_2.joblib')"
      ],
      "metadata": {
        "id": "3wewl0cLLYYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "joblib.dump(model, 'Model_Epoch_5_3000.joblib')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PinWs-VDqLC",
        "outputId": "6029dbbd-0fde-4ea7-ccb7-5c8f15787356"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Model_Epoch_5_3000.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(tokenizer, 'tokenizer_5_3000.joblib')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrJO_8NmGWM3",
        "outputId": "fd501161-4780-4ea0-8390-b3b553bd4673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tokenizer_5_3000.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mQlOrgdUGG_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model_joblib=joblib.load('SecondModel_Epoch_3_1000_2.joblib')"
      ],
      "metadata": {
        "id": "JNdlc7YtLx4G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "069e5c5f-1541-4ca6-d071-562b8cee72ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-af8d2e8e17bc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloaded_model_joblib\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SecondModel_Epoch_3_1000_2.joblib'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    648\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'SecondModel_Epoch_3_1000_2.joblib'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model_joblib"
      ],
      "metadata": {
        "id": "ffswWlNC7V9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model_joblib"
      ],
      "metadata": {
        "id": "48rFugCaL6Sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib as jb"
      ],
      "metadata": {
        "id": "a2YFZW6lMISB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jb.dump(model,'model_jb.pkl')"
      ],
      "metadata": {
        "id": "OXGm8JGwMlQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = jb.load('model_jb.pkl')"
      ],
      "metadata": {
        "id": "o0Y5jn9qMskS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}