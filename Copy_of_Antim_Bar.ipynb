{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhishek-paghdal/Content_Generation/blob/main/Copy_of_Antim_Bar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UmroclTq9zq"
      },
      "outputs": [],
      "source": [
        "# tf.config.set_visible_devices(tf.config.experimental.list_physical_devices('GPU')[0], 'GPU')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQT-GQdnUFa6",
        "outputId": "25dbd8a9-1ac0-4585-a4ca-0f093cacc53d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "D21mvRsi7ifK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "xIeDzCbb7Yju"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/Content_Project/data.csv')"
      ],
      "metadata": {
        "id": "kzlOWrmVr0ty"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "ziMzL_X7r7HY",
        "outputId": "1f071e49-2d6f-44bb-aad8-7dc490653def"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                    _id  \\\n",
              "0  {'$oid': '600837a2b5425d67582e0d8a'}   \n",
              "1  {'$oid': '600837a3b5425d67582e0d8e'}   \n",
              "2  {'$oid': '600837a4b5425d67582e0d91'}   \n",
              "3  {'$oid': '600837a4b5425d67582e0d92'}   \n",
              "4  {'$oid': '600837a5b5425d67582e0d97'}   \n",
              "\n",
              "                                               title        datePublished  \\\n",
              "0  EU lawmakers to push audio-visual sector on ge...  2020-11-30 05:46:35   \n",
              "1  Construction startup Scaled Robotics raises a ...  2020-02-03 07:13:18   \n",
              "2  Roblox buys digital avatar startup Loom.ai â€“ T...  2020-12-14 11:10:15   \n",
              "3  ClickUp hits $1 billion valuation in $100M Ser...  2020-12-15 16:30:14   \n",
              "4  Spotify launches video podcasts worldwide, sta...  2020-07-21 07:13:39   \n",
              "\n",
              "                                         description  \\\n",
              "0  European Union lawmakers are considering wheth...   \n",
              "1  Industrial robots are expensive. But, then, so...   \n",
              "2  Roblox announced today that itâ€™s buying a digi...   \n",
              "3  Just six month after raising its first bit of ...   \n",
              "4  Spotify today announced the global launch of v...   \n",
              "\n",
              "                                             article  \\\n",
              "0  European Union lawmakers are considering wheth...   \n",
              "1  Industrial robots are expensive. But, then, so...   \n",
              "2  Roblox announced today that itâ€™s buying a digi...   \n",
              "3  Just six month after raising its first bit of ...   \n",
              "4  Spotify today announced the global launch of v...   \n",
              "\n",
              "                                            keywords  \n",
              "0  audio-visual, eu, geoblocking, Netflix, stream...  \n",
              "1  Battlefield, construction, Norwegian Construct...  \n",
              "2  Avatar, Bitmoji, bitstrips, computing, films, ...  \n",
              "3  Android, ClickUp, Craft Ventures, Georgian Par...  \n",
              "4           Spotify, streaming music, Video, YouTube  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1099012c-b1de-47f6-8a3c-5c388096f2ac\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>_id</th>\n",
              "      <th>title</th>\n",
              "      <th>datePublished</th>\n",
              "      <th>description</th>\n",
              "      <th>article</th>\n",
              "      <th>keywords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>{'$oid': '600837a2b5425d67582e0d8a'}</td>\n",
              "      <td>EU lawmakers to push audio-visual sector on ge...</td>\n",
              "      <td>2020-11-30 05:46:35</td>\n",
              "      <td>European Union lawmakers are considering wheth...</td>\n",
              "      <td>European Union lawmakers are considering wheth...</td>\n",
              "      <td>audio-visual, eu, geoblocking, Netflix, stream...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>{'$oid': '600837a3b5425d67582e0d8e'}</td>\n",
              "      <td>Construction startup Scaled Robotics raises a ...</td>\n",
              "      <td>2020-02-03 07:13:18</td>\n",
              "      <td>Industrial robots are expensive. But, then, so...</td>\n",
              "      <td>Industrial robots are expensive. But, then, so...</td>\n",
              "      <td>Battlefield, construction, Norwegian Construct...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>{'$oid': '600837a4b5425d67582e0d91'}</td>\n",
              "      <td>Roblox buys digital avatar startup Loom.ai â€“ T...</td>\n",
              "      <td>2020-12-14 11:10:15</td>\n",
              "      <td>Roblox announced today that itâ€™s buying a digi...</td>\n",
              "      <td>Roblox announced today that itâ€™s buying a digi...</td>\n",
              "      <td>Avatar, Bitmoji, bitstrips, computing, films, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>{'$oid': '600837a4b5425d67582e0d92'}</td>\n",
              "      <td>ClickUp hits $1 billion valuation in $100M Ser...</td>\n",
              "      <td>2020-12-15 16:30:14</td>\n",
              "      <td>Just six month after raising its first bit of ...</td>\n",
              "      <td>Just six month after raising its first bit of ...</td>\n",
              "      <td>Android, ClickUp, Craft Ventures, Georgian Par...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>{'$oid': '600837a5b5425d67582e0d97'}</td>\n",
              "      <td>Spotify launches video podcasts worldwide, sta...</td>\n",
              "      <td>2020-07-21 07:13:39</td>\n",
              "      <td>Spotify today announced the global launch of v...</td>\n",
              "      <td>Spotify today announced the global launch of v...</td>\n",
              "      <td>Spotify, streaming music, Video, YouTube</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1099012c-b1de-47f6-8a3c-5c388096f2ac')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1099012c-b1de-47f6-8a3c-5c388096f2ac button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1099012c-b1de-47f6-8a3c-5c388096f2ac');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "1JQvsT44r_L9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1=df[:500]"
      ],
      "metadata": {
        "id": "-BGCUh5qsGUE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.to_csv('data1.csv',index=False)"
      ],
      "metadata": {
        "id": "FUq2eKfXsMV7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv('data1.csv')"
      ],
      "metadata": {
        "id": "1wXKrlHUv8YD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOBjvWgvwMHf",
        "outputId": "18a42a2d-8898-45c8-f9c9-b2e3a8e9584c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split the dataset into train and evaluate**"
      ],
      "metadata": {
        "id": "9QyfG0fuvk2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data, eval_data = train_test_split(data, test_size=0.2)"
      ],
      "metadata": {
        "id": "9eLn5dNLvrID"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nM6EwJs3wrL-",
        "outputId": "36497892-c6cc-4534-d8ba-faf49972dce3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.to_csv('train_data.csv')\n",
        "eval_data.to_csv('eval_data.csv')\n"
      ],
      "metadata": {
        "id": "zPvJgszewvfC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# this is for the train data\n",
        "\n",
        "# Open the CSV file and read the contents\n",
        "with open('/content/train_data.csv', 'r', encoding='utf-8') as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    data = [row['article'] for row in reader]\n",
        "\n",
        "# Write the contents to a text file\n",
        "with open('/content/train_dataset.txt', 'w', encoding='utf-8') as txtfile:\n",
        "    txtfile.write('\\n'.join(data))\n"
      ],
      "metadata": {
        "id": "zWhtJPb6rQeC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import csv\n",
        "\n",
        "# this is for the evaluate data\n",
        "\n",
        "# Open the CSV file and read the contents\n",
        "with open('/content/eval_data.csv', 'r', encoding='utf-8') as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    data = [row['article'] for row in reader]\n",
        "\n",
        "# Write the contents to a text file\n",
        "with open('/content/eval_dataset.txt', 'w', encoding='utf-8') as txtfile:\n",
        "    txtfile.write('\\n'.join(data))\n"
      ],
      "metadata": {
        "id": "fTOqWtRMxB6d"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKe_TIrZtQ7U",
        "outputId": "5fee53da-8903-4faf-a090-c4ce5327ea78"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.4)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n"
      ],
      "metadata": {
        "id": "D2rcW8IEtEh0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained GPT-2 tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n"
      ],
      "metadata": {
        "id": "72GRRK53thcU"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare your dataset by creating a TextDataset object\n",
        "train_dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path='/content/train_dataset.txt',\n",
        "    block_size=128\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teLEUf84tmnS",
        "outputId": "b1780c38-87d9-42ef-eb92-2208f12776c7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare your dataset by creating a TextDataset object\n",
        "eval_dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path='/content/eval_dataset.txt',\n",
        "    block_size=128\n",
        ")\n"
      ],
      "metadata": {
        "id": "6XwMGM9axeRF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pytorch-lightning[callbacks]\n"
      ],
      "metadata": {
        "id": "TxDwB5Rgsjfi",
        "outputId": "5f56b89f-766f-4448-b475-0cbd21f66e1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch-lightning[callbacks] in /usr/local/lib/python3.9/dist-packages (2.0.1)\n",
            "\u001b[33mWARNING: pytorch-lightning 2.0.1 does not provide the extra 'callbacks'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning[callbacks]) (23.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning[callbacks]) (0.8.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning[callbacks]) (2.0.0+cu118)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning[callbacks]) (0.11.4)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning[callbacks]) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning[callbacks]) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning[callbacks]) (4.5.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning[callbacks]) (6.0)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning[callbacks]) (2023.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning[callbacks]) (2.27.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.9/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning[callbacks]) (3.8.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.11.0->pytorch-lightning[callbacks]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.11.0->pytorch-lightning[callbacks]) (2.0.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.11.0->pytorch-lightning[callbacks]) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.11.0->pytorch-lightning[callbacks]) (3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.11.0->pytorch-lightning[callbacks]) (3.10.7)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning[callbacks]) (16.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning[callbacks]) (3.25.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning[callbacks]) (1.8.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning[callbacks]) (6.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning[callbacks]) (22.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning[callbacks]) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning[callbacks]) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning[callbacks]) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning[callbacks]) (2.0.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.11.0->pytorch-lightning[callbacks]) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning[callbacks]) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning[callbacks]) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning[callbacks]) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.11.0->pytorch-lightning[callbacks]) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "class MyModelCheckpoint(ModelCheckpoint):\n",
        "    def on_init_end(self, trainer):\n",
        "        super().on_init_end(trainer)"
      ],
      "metadata": {
        "id": "ahNKnteYsZyk"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model checkpoint\n",
        "checkpoint_callback = MyModelCheckpoint(\n",
        "    dirpath='./checkpoints',\n",
        "    filename='checkpoint-{epoch:02d}',\n",
        "    save_top_k=3,\n",
        "    monitor='loss',\n",
        "    mode='min'\n",
        ")\n"
      ],
      "metadata": {
        "id": "f-Dr1L6trO-V"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the trainer with the appropriate training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy='epoch',\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    num_train_epochs=5,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    data_collator=data_collator,\n",
        "    eval_dataset=eval_dataset,\n",
        "   # callbacks=[checkpoint_callback]\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "fD9QozNhttZB"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load the pre-trained model and tokenizer\n",
        "model_name = 'gpt2'\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Define your custom prompt\n",
        "prompt = \"Artificial intelligence is\"\n",
        "\n",
        "# Define the maximum length of each segment\n",
        "max_segment_length = 1024\n",
        "\n",
        "# Define the total length of the final article\n",
        "total_length = 5000\n",
        "\n",
        "# Generate the first segment of text\n",
        "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "generated_text = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_length=max_segment_length,\n",
        "    num_beams=5,\n",
        "    no_repeat_ngram_size=2,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "# Decode the generated text and print it\n",
        "generated_text = tokenizer.decode(generated_text[0], skip_special_tokens=True)\n",
        "print(generated_text)\n",
        "\n",
        "# Generate additional segments of text\n",
        "while len(generated_text) < total_length:\n",
        "    # Get the last 1024 tokens of the previous segment as the new prompt\n",
        "    prompt = generated_text[-max_segment_length:]\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "    \n",
        "    # Generate the next segment of text\n",
        "    next_segment = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_length=max_segment_length,\n",
        "        num_beams=5,\n",
        "        no_repeat_ngram_size=2,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    \n",
        "    # Decode the generated text and append it to the previous segment\n",
        "    next_segment = tokenizer.decode(next_segment[0], skip_special_tokens=True)\n",
        "    generated_text += next_segment\n",
        "\n",
        "# Print the final generated text\n",
        "print(generated_text)\n"
      ],
      "metadata": {
        "id": "aqtnLBv6tBFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune the model on your dataset\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "id": "dDQ6Mbrgt1UP",
        "outputId": "93a31106-ceec-4aeb-8940-9dcd262ff984"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='855' max='855' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [855/855 08:36, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.507422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.482466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.444200</td>\n",
              "      <td>3.478778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.444200</td>\n",
              "      <td>3.480862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.444200</td>\n",
              "      <td>3.485349</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=855, training_loss=3.318068690606725, metrics={'train_runtime': 517.7165, 'train_samples_per_second': 26.366, 'train_steps_per_second': 1.651, 'total_flos': 891659059200000.0, 'train_loss': 3.318068690606725, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "prIthD5_W1Gm",
        "outputId": "83355f0d-f2d7-4e2c-c444-83e6b4fbc253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='464' max='464' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [464/464 00:39]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 3.298492670059204,\n",
              " 'eval_runtime': 39.8979,\n",
              " 'eval_samples_per_second': 92.937,\n",
              " 'eval_steps_per_second': 11.63,\n",
              " 'epoch': 5.0}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer"
      ],
      "metadata": {
        "id": "j2m1KtL2W9Us",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "081572f9-0d44-4d31-9dc9-680eee88d765"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<transformers.trainer.Trainer at 0x7f49064843d0>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJmPE27_DT5u",
        "outputId": "5851c403-053c-48ec-9f61-72569b7d4805"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (2.0.0+cu118)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch) (3.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch) (3.10.7)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch) (16.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "AEj0iM-_DWjs"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.to('cuda')a# Convert the model to CPU\n",
        "\n",
        "model = model.to(torch.device('cpu'))\n"
      ],
      "metadata": {
        "id": "b57VIFZN_T4Y"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate text using the fine-tuned model\n",
        "prompt = 'which is the best phone that i can buy ?'\n",
        "generated_text = model.generate(\n",
        "    input_ids=tokenizer.encode(prompt, return_tensors='pt'),\n",
        "    max_length=200,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    temperature=1.0,\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(generated_text[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5dUnEdWt6nP",
        "outputId": "31ec0106-5c6e-4211-ddb5-a53bb311cc1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "which is the best phone that i can buy?) for just $99. And then, with the exception of the base model, only $34 when it comes to hardware. The iPhone 5 is coming in two models, one with $799 in cash and one with $999 in cash, but with both starting at $1,599 in cash and $999 in cash.  The iPhone SE has $999 and $999. The iPhone 12 is coming in two versions with the base model and the entry-level models with the entry-level model with the entry-level model. If youâ€™re new to the line of iPhones and want to upgrade in price, this is your option. If youâ€™ve made it that far in your life and have a budget, this isnâ€™t the phone for you.  Buy your iPhone SE  and the iPhone SE   are the best and the best phones for you in the long run. The  iPhone  is not\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Tokenize the prompt and convert it to input ids\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "    \n",
        "    # Generate the next segment of text\n",
        "    outputs = model.generate(input_ids=input_ids, max_length=max_segment_length, do_sample=True)\n",
        "    segment = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    # Append the segment to the generated text\n",
        "    generated_text += segment\n",
        "    \n",
        "    # Get the last 1024 tokens of the previous segment as the new prompt\n",
        "    prompt = generated_text[-max_segment_length:]"
      ],
      "metadata": {
        "id": "CiT2-uCK0_bJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate text using the fine-tuned model\n",
        "prompt = 'Artificial Intelligence'\n",
        "generated_text = model.generate(\n",
        "    input_ids=tokenizer.encode(prompt, return_tensors='pt').to('cuda'),\n",
        "    max_length=200,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    temperature=1.0,\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(generated_text[0], skip_special_tokens=True))\n",
        "\n",
        "\n",
        "# Define the total length of the final article\n",
        "total_length = 5000\n",
        "max_segment_length=1024\n",
        "\n",
        "#print(tokenizer.decode(generated_text[0], skip_special_tokens=True))\n",
        "\n",
        "# Generate additional segments of text\n",
        "while len(generated_text) < total_length:\n",
        "    # Get the last 1024 tokens of the previous segment as the new prompt\n",
        "    prompt =generated_text[-max_segment_length:]\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to('cuda')\n",
        "    \n",
        "    # Generate the next segment of text\n",
        "    next_segment = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_length=max_segment_length,\n",
        "        num_beams=5,\n",
        "        no_repeat_ngram_size=2,\n",
        "        early_stopping=True\n",
        "    )\n",
        "    \n",
        "    # Decode the generated text and append it to the previous segment\n",
        "    next_segment = tokenizer.decode(next_segment[0], skip_special_tokens=True)\n",
        "    generated_text += next_segment\n",
        "\n",
        "# Print the final generated text\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "NLYV71OUtcJ8",
        "outputId": "75b1b62d-67f8-4e76-89d3-8eb052de7ae8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 862
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificial Intelligence and its future, â€œwill continue to evolve in the near and distant future to enable a whole new type of computing world that will revolutionize the way our society operates.â€ The event was organized by  the Stanford Artificial Intelligence Association. It also included a virtual networking event called the Intercom Summit that brought together experts from the AI community, startups, researchers, and technologists.\n",
            "Uber is set to introduce its new service, Voucher, to its U.S. fleets beginning this fall. A Voucher ride-hailing service will launch at the start of 2019, but Uber says it will have its first fleet by late 2021. The company has so far declined to comment on when Voucher will be available in the U.S. â€” and the exact date for the rollout remains to be determined. The firm initially began developing a service through Uber in the U.S., but found its business in Southeast Asia and the Middle East. The product\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-c1e401c0f298>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Get the last 1024 tokens of the previous segment as the new prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mgenerated_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmax_segment_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Generate the next segment of text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2299\u001b[0m                 method).\n\u001b[1;32m   2300\u001b[0m         \"\"\"\n\u001b[0;32m-> 2301\u001b[0;31m         encoded_inputs = self.encode_plus(\n\u001b[0m\u001b[1;32m   2302\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2303\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2707\u001b[0m         )\n\u001b[1;32m   2708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2709\u001b[0;31m         return self._encode_plus(\n\u001b[0m\u001b[1;32m   2710\u001b[0m             \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2711\u001b[0m             \u001b[0mtext_pair\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m             )\n\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m         \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    633\u001b[0m                     )\n\u001b[1;32m    634\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    636\u001b[0m                         \u001b[0;34mf\"Input {text} is not valid. Should be a string, a list/tuple of strings or a list/tuple of\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m                         \u001b[0;34m\" integers.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input tensor([[ 8001,  9542,  9345,   290,   663,  2003,    11,   564,   250, 10594,\n          2555,   284, 18101,   287,   262,  1474,   290, 12899,  2003,   284,\n          7139,   257,  2187,   649,  2099,   286, 14492,   995,   326,   481,\n          5854,  1096,   262,   835,   674,  3592, 14051,    13,   447,   251,\n           383,  1785,   373,  8389,   416,   220,   262, 13863, 35941,  9345,\n          5396,   764,   632,   635,  3017,   257,  7166, 19140,  1785,  1444,\n           262,  4225,   785, 20014,   326,  3181,  1978,  6154,   422,   262,\n          9552,  2055,    11, 21685,    11,  4837,    11,   290,  1579,  9251,\n            13,   198, 39018,   318,   900,   284, 10400,   663,   649,  2139,\n            11,   569,   280,  2044,   837,   284,   663,   471,    13,    50,\n            13, 39439,  3726,   428,  2121,    13,   317,   569,   280,  2044,\n          6594,    12,    71, 11608,  2139,   481,  4219,   379,   262,   923,\n           286, 13130,    11,   475, 12024,  1139,   340,   481,   423,   663,\n           717, 11026,   416,  2739, 33448,    13,   383,  1664,   468,   523,\n          1290,  7392,   284,  2912,   319,   618,   569,   280,  2044,   481,\n           307,  1695,   287,   262,   471,    13,    50,    13,   851,   290,\n           262,  2748,  3128,   329,   262, 38180,  3793,   284,   307,  5295,\n            13,   383,  4081,  7317,  2540,  5922,   257,  2139,   832, 12024,\n           287,   262,   471,    13,    50,  1539,   475,  1043,   663,  1597,\n           287, 21199,  7229,   290,   262,  6046,  3687,    13,   383,  1720]],\n       device='cuda:0') is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Set the maximum length of each segment\n",
        "max_segment_length = 1024\n",
        "\n",
        "# Set the initial prompt\n",
        "prompt = \"Artificial intelligence\"\n",
        "\n",
        "# Initialize the generated text\n",
        "generated_text = prompt\n",
        "\n",
        "# Generate segments of text and stitch them together\n",
        "while len(generated_text) < 500:\n",
        "    # Encode the prompt as input IDs\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "\n",
        "    # Generate the next segment of text\n",
        "    output = model.generate(input_ids=input_ids.to('cuda'),\n",
        "                             max_length=max_segment_length,\n",
        "                             do_sample=True,\n",
        "                             top_p=0.95,\n",
        "                             top_k=50)\n",
        "    \n",
        "    # Decode the output to text\n",
        "    segment = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Add the segment to the generated text\n",
        "    generated_text += segment\n",
        "\n",
        "    # Get the last 1024 tokens of the previous segment as the new prompt\n",
        "    prompt = generated_text[-max_segment_length:]\n",
        "\n",
        "# Print the generated text\n",
        "print(generated_text)\n",
        "final_text=generated_text\n"
      ],
      "metadata": {
        "id": "mccuNnok39gB",
        "outputId": "eeca57db-69c2-4413-c29c-31afe7156d7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Artificial intelligenceArtificial intelligence has taken a big step toward giving us the tools and technologies to solve a number of  fundamental problems for society today.  Itâ€™s now clear from the start that AI is about making sense of whatâ€™s going on within the context of whatâ€™s going on at home, as well. And todayâ€™s revelations have added even more fuel to that fire. AI is also increasingly being used and described by various  corporate  companies and governments  to manipulate and exploit the general population without anyoneâ€™s knowledge. While the world is still reeling in the devastation caused by pandemic, itâ€™s not the  rest of the world that hasnâ€™t started noticing the COVID-19 trend. At the same time, as I understand, the pandemic has already killed tens of thousands of people and caused millions of dollars in social unrest that has taken root in other parts of the world, all with the explicit aim of reducing the spread of disease to all of us. The same is true of other aspects of our lives, such as the coronavirus pandemic and the global pandemic. In fact, there are new COVID-19 coronavirus pandemics coming to light,â€ writes a  New York Times piece last year. But COVID-19 is very well-known in the world of digital advertising. Just as well-known are the digital giants that have been feeding data into machine learning systems to keep tabs on the flow of eyeballs and the movement of people.  In the context of todayâ€™s revelations, the  DPCâ€™s decision to set up a joint research unit to study the issue would have been a clear, strategic direction for the agency, even if the public health outlook remains somewhat bleak (think of it as having little impact at all, if at all). In any case, the threat from AI is real: Itâ€™s not just that most of us  have grown up believing that things are easy and that government canâ€™t protect us from the dangers that we might face. Itâ€™s actually that we are increasingly losing our sense of control over what goes on inside our heads. AI has been  driving social engineering innovations  as the U.S. government  aggressively  pushes online censorship of dissent,  including COVID-19, to make us pay attention to and avoid critical conversations. This isnâ€™t the first time that the DPC has had its act together on this issue. Last summer, it  set up a parallel partnership with the World Health Organization  to examine how AI might impact peopleâ€™s physical and emotional health. The collaboration â€” with  TikTok    and the World Health Organization  â€” brought together tech experts, such as Andreessen Horowitzâ€™s Henry Fried, and researchers from the U.K. and Canada and Germany, to figure out what we were doing online, how we were impacting the next generation of people through misinformation and misinformation, and what tools we might be able to use to limit these trends. In the ensuing years, the partnership has accelerated in terms of what is now the technologyâ€™s dominant focus: artificial intelligence as the next major consumer or health technology â€” along with AI-as-a-service (AIAA) â€” which we know will be coming online soon. Itâ€™s worth noting, though, that todayâ€™s developments are likely due to what many feel is a single, ongoing trend. COVID-19 is a looming pandemic that is coming to the home of the DPC as one of the main challenges facing modern society â€” the coronavirus crisis, for example â€” as a whole. But todayâ€™s news raises the possibility that the problem is spreading through a wider set of trends, as AI-as-a-service (AIAA) continues to become more common. The spread of these trends, however, could pose even more challenges for the DPC. The threat from AI could not have been more serious. For now, however, the problem that still exists, and the potential for it to affect our society, is that we cannot ignore what we really want out of life in a world where artificial intelligence has become pervasive and which we expect will evolve over time. It also presents a stark opportunity for society in which we must come together more rapidly to address the impact of the pandemic â€” by building on our common principles about a responsible digital landscape that helps people make informed choices and avoid misinformation. And a whole slew of new technologies are  making the journey closer and harder to accomplish in light of this pandemic â€” from drones to better public health monitoring to new ways to make decisions, to cloud computing, to digital health. What might be even more important than the fact that most of these developments are already happening (especially given the challenges posed by the pandemic), is that most of these changes would also lead to a new era in which we can really start to move more quickly â€” and not just to the detriment of\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Set up API endpoint and headers\n",
        "url = \"https://text-monkey-summarization.p.rapidapi.com/summarize\"\n",
        "headers = {\n",
        "    \"X-RapidAPI-Host\": \"text-monkey-summarization.p.rapidapi.com\",\n",
        "    \"X-RapidAPI-Key\": \"YOUR_RAPIDAPI_KEY\",\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Accept\": \"application/json\"\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "UniV6XcJ7KDk"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input text\n",
        "input_text = \"In the digital era, the explosion of technology has been matched by an equally seismic shift in the ways we think and talk about it. New digital tools give rise to new terms and phrases, as well as new conceptual frameworks for understanding how these tools affect and interact with society. As we contend with novel and heightened forms of harm, words like disinformation and doxing have entered the mainstream. And concepts like digital rights and data governance have come into existence amidst the push to align the technology of today with our vision for a better future.  \"\n",
        "\n",
        "# Define the summarization parameters\n",
        "params = {\n",
        "    \"text\": input_text,\n",
        "    \"sent_count\": 3,   # Number of sentences in the summary\n",
        "    \"percentage\": 30  # Percentage of the original text to include in the summary\n",
        "}\n",
        "\n",
        "# Send the API request\n",
        "response = requests.post(url, headers=headers, data=json.dumps(params))\n",
        "\n",
        "# Extract the summary from the response\n",
        "summary = response.json()[\"summary\"]\n",
        "\n",
        "# Print the summary\n",
        "print(summary)\n"
      ],
      "metadata": {
        "id": "0yaL1hFO7N9Q",
        "outputId": "7d1d06af-8a24-4e2b-ebcc-ca43d2dc0d1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-4500766eaf40>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Extract the summary from the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"summary\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Print the summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'summary'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Set up the API endpoint\n",
        "endpoint = \"https://api.textgears.com/summarize\"\n",
        "\n",
        "# Set up the parameters for the API request\n",
        "params = {\n",
        "    \"text\": \"This is the text to be summarized.\",\n",
        "    \"sentences_number\": 3,\n",
        "    \"key\": \"YOUR_API_KEY\"\n",
        "}\n",
        "\n",
        "# Send the API request\n",
        "response = requests.get(endpoint, params=params)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PWlV1okt7_82",
        "outputId": "beb5af24-19d6-493b-a488-516f19c63b12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-4d0767e2864e>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Extract the summary from the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"result\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Print the summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'result'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the summary from the response\n",
        "summary = response.json()[\"result\"]\n",
        "\n",
        "# Print the summary\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "MI9GV7x29bLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.save_model('Second_Model_GPT2-Epoch_3_1000')"
      ],
      "metadata": {
        "id": "-GZrLfH4X2G9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import zipfile\n",
        "# zip_ref = zipfile.ZipFile(\"Model_2_GPT-2_Epoch_3_1000.zip\", \"r\")\n",
        "# zip_ref.extractall()\n",
        "# zip_ref.close()"
      ],
      "metadata": {
        "id": "QJUIMeuH_ugz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# joblib.dump(model, 'SecondModel_Epoch_3_1000_2.joblib')"
      ],
      "metadata": {
        "id": "3wewl0cLLYYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "joblib.dump(model, 'Model_Epoch_5_3000.joblib')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PinWs-VDqLC",
        "outputId": "0cef35ad-ce79-455c-afa1-2dd5cd296b2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Model_Epoch_5_3000.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(tokenizer, 'tokenizer_5_3000.joblib')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrJO_8NmGWM3",
        "outputId": "aa792995-ecc7-439e-ea93-103c933799b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tokenizer_5_3000.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mQlOrgdUGG_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model_joblib=joblib.load('SecondModel_Epoch_3_1000_2.joblib')"
      ],
      "metadata": {
        "id": "JNdlc7YtLx4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model_joblib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffswWlNC7V9f",
        "outputId": "f59d9079-2761-44aa-c9de-90ce6dabd206"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model_joblib"
      ],
      "metadata": {
        "id": "48rFugCaL6Sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib as jb"
      ],
      "metadata": {
        "id": "a2YFZW6lMISB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jb.dump(model,'model_jb.pkl')"
      ],
      "metadata": {
        "id": "OXGm8JGwMlQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = jb.load('model_jb.pkl')"
      ],
      "metadata": {
        "id": "o0Y5jn9qMskS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate text using the fine-tuned model\n",
        "prompt = 'which is the best phone that i can buy ?'\n",
        "generated_text = loaded_model.generate(\n",
        "    input_ids=tokenizer.encode(prompt, return_tensors='pt').to('cuda'),\n",
        "    max_length=200,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    temperature=1.0,\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(generated_text[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "RT6AQ776M9eh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}